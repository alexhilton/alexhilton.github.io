<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Deeplearning | 稀有猿诉]]></title>
  <link href="http://toughcoder.net/blog/categories/deeplearning/atom.xml" rel="self"/>
  <link href="http://toughcoder.net/"/>
  <updated>2024-08-18T13:03:32+08:00</updated>
  <id>http://toughcoder.net/</id>
  <author>
    <name><![CDATA[Alex Hilton]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[优雅的训服大模型：深入浅出Prompt技巧]]></title>
    <link href="http://toughcoder.net/blog/2024/05/07/prompt-made-easy/"/>
    <updated>2024-05-07T22:30:17+08:00</updated>
    <id>http://toughcoder.net/blog/2024/05/07/prompt-made-easy</id>
    <content type="html"><![CDATA[<p>ChatGPT以降，彻底引爆了AI，大模型进入了公众视野，每个人都可以享受AI带来的乐趣。大语言模型（Large Language Model LLM）虽然可以达到「类人」的水平，但仍需要以「大模型角度」去思考和对话才能最大限度的利用大模型的威力。这便是Prompt Engineer需要做的事情。今天就让我们学习总结一些Prompt技巧，以能优雅的训服大模型。</p>

<p><a href=""><img src="https://japanesetarheel.com/wp-content/uploads/2023/04/prompt-engineering.jpg" title="auto auto" ></a></p>

<!-- more -->


<p>说起来挺吓人的，但其实也没那么复杂，只要稍学习一下，厘清LLM的本质，就能掌握到Prompt的精髓，从而就能大大提高使用LLM的效率。</p>

<h2>本质是提问的技巧</h2>

<p>大模型第一次达到了「类人」水平，用户可以用自然语言与计算机进行交流了。那么为了达到事半功倍的效果，我们就要精进提问的技巧，这便是Prompt Engineering。如何做呢？要把大模型视为精通某一领域的专业顾问，要以与专业顾问（比如导游，咨询师）对话的角度来看待问题。</p>

<p>比如说马上五一假期了，想要去上海玩，你问大模型『五一假期去上海玩，推荐一些行程』，大模型肯定会给你一个中规中矩的万能日程。这不能怪大模型，如果你如此问一个导游，负责的导游会直接反问你一大堆细节问题，因为这是行程所必须的；或者也是随便推荐一些大家都知道的地方。但肯定 是没有参考价值的。</p>

<p><img src="https://leadingwithquestions.com/wp-content/uploads/2021/09/ArtOfAskingPowerfulQuestions-e1630504910641.jpg" alt="" /></p>

<p>为了达到最好的「沟通」效果，避免「Garbage In, Garbage Out」，就需要：</p>

<ul>
<li>视为在与专业人士对话，也就是要拟定大模型的角色</li>
<li>一次对话尽可能的专注于一个问题或者一个主题</li>
<li>把问题描述清楚，需要的关键要素都列清楚，比如时间，地点人物，关键事件等等</li>
<li>限定问题，也就是说要尽可能多的补充问题细节，限定问题需要的回答，比如说可以做什么，不可以做什么，需要什么是时间点</li>
<li>恰当的分隔，要多多使用标点符号对提问进行分隔，这样更有利于大模型抓住重点</li>
</ul>


<p>可以发现，这其实是沟通的技巧，抛开大模型，我们与正常的人沟通时，不也应该这样做吗？平时里的闲聊除外，正式的与人沟通时也应该使用这些技巧以达到最好的沟通效果。所以，最先需要掌握就是沟通技巧中的提问技巧。</p>

<p>继续我们上面的问题，使用上面的方法来优化提示：『五一假期，从南京出发，坐高铁，目的地上海，亲子3日游，不去迪士尼，不去动物园，安排详细行程』。这回得到的回答肯定有更大的参考价值。</p>

<h2>及时补充上下文</h2>

<p>大语言模型较以往的AI最大的进步在于超长的上下文记忆能力，这是它能达到「类人」水平的最主要的原因。那么在与大模型进行对话的时候，就要及时的补充上下文信息。一方面，你不可能一次性的把提问信息全都写全写对，那么一旦想到新的提示，就要及时的给到大模型；另外一方面，大模型有时候会胡编乱造，甚至胡言乱语（大模型都有一个叫做温度temperature的参数以控制这方面的行为），这并不是bug，而是语言创造力的一个体现。那么，一旦发现大模型跑偏了，就需要及时提供新的提示，补充上下文信息，对大模型进行纠正。</p>

<p>另外，就是如果感觉问题不太好理解，或者对输出有特殊的要求，还有一个补充上下文的办法就是给一个输出的示例，大模型是能够很好的捕获这一点的，并且这个对让大模型输出优质的回答非常有帮助。</p>

<p>不用担心溢出哈（就是输入字数太多，导致大模型理解不了），都4202年了，现在的大模型的上下文能力至少在4096个Token以上，对于大部分的常规问题来说足够了。</p>

<h2>掌握常见的命令和特殊标记</h2>

<p>大模型的输入是自然语言，但计算机毕竟是程序化的机器，那么就会有一些特殊的命令和标记以代表一些常用的功能。这个其实不是大模型所特有的，凡是接收字符串输入的地方，都会有一些特殊的命令和标记，如搜索引挚，如一些聊天应用。</p>

<p>特殊命令和特殊标记的作用在于简化输入，提升效率，特别是对于一些复杂的问题，比如像专业领域的问题，标准化的标记能大大的加强共识程度。这也跟大模型关系不大，就比如说数学和物理学上的符号，能达到一符号胜千言。</p>

<p>命令就是简洁，清晰和无歧义的动词：如描述，拟定，写出，规划等等。</p>

<p>而特殊标记则像方括号[]，三个引号&#8221;&ldquo;&#8221;，三个&#35;&#35;&#35;等等。</p>

<h2>理解专业领域知识</h2>

<p>前面的几点都是一些通用的提示优化小技巧，适用于常规的通用的小问题。如果是专业领域的问题，光有以上的技巧明显是不够的。这时就需要领域知识了，对领域要有深刻的理解，并熟悉领域的专业术语，把抽象的，泛化的问题或者需求，用专业术语进行描述，甚至对领域进行建模，转化为详细的，具体的提示词。然后输入到大模型，由大模型进行求解或者实现。最为典型的两个领域就是代码生成和图像生成，这是两个专业性比较强的领域，对结果的要求也比较高，因此对提示词的要求也更高。</p>

<h3>Code prompts</h3>

<p>要想让大模型写出更好的代码，就必须尽可能详细的给出代码要素，例如：编程语言必须指明，输出输出的参数或者格式必须指明，把限制说清楚，比如不能用什么，比如时间空间复杂度的限制，最重要的就是把需求写清楚，也就是要实现什么功能的代码。</p>

<p>扩展阅读：</p>

<ul>
<li><a href="https://www.rtcdeveloper.cn/cn/community/blog/26205">GitHub Copilot 教程：提示词、技巧和用例</a></li>
<li><a href="https://github.blog/2023-06-20-how-to-write-better-prompts-for-github-copilot/">How to use GitHub Copilot: Prompts, tips, and use cases</a></li>
<li><a href="https://code.tutsplus.com/tips-for-effective-code-generating-chatgpt-prompts-prompt-design--cms-107346t">Tips for Effective Code-Generating ChatGPT Prompt Design</a></li>
</ul>


<h3>图像生成</h3>

<p>对于像Stable Diffusion的AIGC，更是需要一些特定领域的专业知识，才能写出比较好的提示词，达到预期的效果。比如提示词中要包含艺术风格，尺寸，比例，色彩，线条等等，而且要是关键词式的。所以，已经有<a href="https://github.com/lllyasviel/ControlNet">一些工具</a>，直接给出各种领域参数的可选择的具体值，以代替文本输入框，只需要在众多的参数的值中选择后，即可生成图片，确实方便很多。</p>

<p>扩展阅读：</p>

<ul>
<li><a href="https://juejin.cn/post/7273025863989755956">Stable Diffusion 提示词入门指南</a></li>
<li><a href="https://docs.midjourney.com/docs/explore-prompting">Explore Prompting</a></li>
</ul>


<h2>优质的资源</h2>

<p>Prompting是有固定的套路可循的，就好比写代码，不用重复造轮子，已经有很多优质的prompt模板了。学会坐在现成的轮子上面，不但可以提高效率事半功倍，且能走的更稳更远。</p>

<ul>
<li><a href="https://github.com/PlexPt/awesome-chatgpt-prompts-zh">ChatGPT 中文调教指南</a></li>
<li><a href="https://github.com/logikon-ai/awesome-deliberative-prompting">Awesome Deliberative Prompting</a></li>
<li><a href="https://github.com/f/awesome-chatgpt-prompts">Awesome ChatGPT Prompts</a></li>
</ul>


<p>GitHub是个座金矿，每当需要什么资源的时候就去搜索「awesome xxx」就能得到非常优质的相关资源合集，这比自己一个一个的去找要方便太多了。</p>

<h2>Prompt Engineer工具</h2>

<p>提示工程Prompt Engineering已经成为了一个新的专业，有专职的职位叫做提示工程师Prompt Engineer，它们需要对领域有深刻理解，把抽象的，泛化的需求，描述为具体的提示词以让大模型去求解或者实现。除了提示工程师外，也有专门的提示工具，用以生成提示词，或者帮助做提示优化，当不知道如何提示时，或者想优化提示词时，就可以使用这些工具来帮忙：</p>

<ul>
<li><a href="https://prompt.com/">prompt.com</a></li>
<li><a href="https://promptmetheus.com/">Prompt Engineering IDE</a></li>
<li><a href="https://github.com/microsoft/promptbench">PromptBench</a></li>
</ul>


<h2>参考资料</h2>

<ul>
<li><a href="https://www.promptingguide.ai/">Prompt Engineering Guide</a></li>
<li><a href="https://gptpmt.com/">GPT Prompt</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[教你用最优雅的方式把玩大模型]]></title>
    <link href="http://toughcoder.net/blog/2024/05/01/run-llm-locally/"/>
    <updated>2024-05-01T23:04:34+08:00</updated>
    <id>http://toughcoder.net/blog/2024/05/01/run-llm-locally</id>
    <content type="html"><![CDATA[<p><a href="https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/">人工智能（Artificial Intelligence）</a>几乎与计算机科学一样古老，在二十世纪50年代被首次提出，在60年代就已经形成比较成熟的理论。但受制于算力和数据，直到二十一世纪第一个10年后才有了重大突破（深度学习和CNN），并在第二个10年正式爆发（大模型）。2022年秋OpenAI的ChatGPT 3横空出世，让AI第一次达到「类人」层次，大语言模型（Large Language Model, LLM）也正式进入了公众的视野。自此，大模型开始刷屏和霸榜，人们言必之大模型。如果不折腾折腾大模型，似乎就是原始人，跟别人聊天都插不上话。痛定思痛，今天就来好好研究下大模型以跟上步伐。</p>

<p><a href=""><img src="https://researchworld.com/uploads/attachments/clr4q0wj95yyf8otdekmzjfy0-large-language-models-for-aspect-based-sentiment-analysis.max.png" title="auto auto" ></a></p>

<!-- more -->


<p>注意：常说的大模型是大语言模型的一种不严谨的简称，更为好的说法是<a href="https://www.cloudflare.com/learning/ai/what-is-large-language-model/">大语言模型（Large Langauge Model）</a>或者用其英文简写LLM，本文可能会混着用。</p>

<p>要了解一个东西最好的方式就先学会使用它，所以我们先从使用大模型开始。</p>

<h2>在本地部署LLM</h2>

<p>体验大模型的方式有很多种，最方便的就是直接使用各大AI大厂提供的聊天机器人如ChatGPT或者ChatGLM。确实很有趣，可以发现LLM与以往的人工智障非常不同的地方在于，它能听懂人话了，并且说的也像人话，也就是说它真的达到了『类人』层次了。</p>

<p>身为开发人猿，光这么把玩太无聊了，最适合开发人猿的玩法就是自己折腾，对的，我们要在本地部署LLM，这样玩起来才更过瘾。省流点说在本地部署有如下好处：</p>

<ul>
<li>可以深入了解LLM的技术栈，亲自折腾一遍才能知道到底有啥，需要啥。</li>
<li>更加安全，且能保护个人隐私。不用多说，直接用Chat服务或者API确实方便，但都是把数据传到别人的服务器上。有些不方便说的话，不适合别人看的，那肯定就不能用了。但使用部署在本地的LLM就不用担心了。</li>
<li>定制LLM以打造个人的知识库或者知识助手。</li>
<li>进行模型微调和深入学习。</li>
<li>积累经验，后续如果上云，有经验了就可以快速部署。</li>
</ul>


<p>在本地部署LLM好处简直不要太多，唯一的缺点就是LLM这玩意儿很费硬件，跑起来比较费钱，要跑的顺畅一些更是大把费钱。</p>

<h2>开源LLM托管平台</h2>

<p>很明显要想本地部署LLM，模型本身必须是开源的，因此只有开源的LLM才能在本地部部署，闭源的模型，只能通过其API使用。</p>

<p>那么，在折腾之前必须要搞懂的事情就是到哪里去找开源的LLM。幸运的是不但现在有很多开源LLM，也有非常方便的LLM托管平台。LLM托管平台就像GitHub之于开源代码一样，各大公司研发和测试完成后就会把LLM发布到托管平台上面，以供人使用。</p>

<p>最为著名的当属<a href="https://huggingface.co/models">HuggingFace</a>了，它不但提供LLM的托管，还有一个几乎成为业界标准的<a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">LLM评估系统</a>，定期发布最新模型的评估，以帮助大家选择合适的LLM。而且还提供了下载和使用LLM的Python库，即<a href="https://pypi.org/project/transformers/">著名的transformers</a>。但很可惜，我们无法访问（你懂得）。</p>

<p>莫慌，对于无法访问的技术网站，一定有<a href="https://hf-mirror.com/">国内的镜像</a>的，非常的好用，而且访问速度很快。</p>

<p><a href="https://mirrors.tuna.tsinghua.edu.cn/">清华大学</a>也曾有镜像，不过后来不能用了。</p>

<h2>如何在本地运行LLM</h2>

<p>下面介绍几种非常方便的，五分钟就能学会的本地部署和运行LLM的方式。</p>

<h3>Ollama</h3>

<p><img src="https://guidady.com/wp-content/uploads/2023/07/Ollama.png" alt="" /></p>

<p>最为方便的方式就是使用<a href="https://ollama.com/">Ollama</a>，它使用起来特别的方便，<a href="https://github.com/ollama/ollama">安装好以后</a>，直接一句就能运行并使用LLM了：</p>

<pre><code class="bash">ollama run llama3
</code></pre>

<p>这就能运行Meta的最新的LLaMA3模型。当然了，运行具体模型前最好先读一读其文档，确认一下硬件配置是否满足模型要求。像家庭比较贫困的笔者用的是乞丐版的MBP（16G RAM+4G GPU）只能选择8B以内的模型，家庭条件比较好的可以上13B的模型，而33B的最好不要试了。</p>

<p>Ollama非常的好用，它本身是C/S式的，也就是说它会启一个小型的HTTP server以运行LLM，除了直接使用Ollama自己的终端以外，也可以充当模型API给其他工具使用，比如像<a href="https://python.langchain.com/docs/get_started/introduction">LangChain</a>就可以无缝对接Ollama。</p>

<p>它的缺点就是它是源于Mac，对Mac最为友好，其他系统如Windows和Linux是后来才支持的。另外就是使用起来比较简陋，仅有一个命令行终端，所以比较好的方式是使用Ollama来管理和运行LLM，但是再用其他工具来构建比较好用的终端。</p>

<p>扩展阅读：</p>

<ul>
<li><a href="https://www.freecodecamp.org/news/how-to-run-open-source-llms-locally-using-ollama/">How to Run Open Source LLMs Locally Using Ollama</a></li>
<li><a href="https://klu.ai/glossary/ollama">Ollama: Easily run LLMs locally</a></li>
<li><a href="https://www.theregister.com/2024/03/17/ai_pc_local_llm/">How to run an LLM on your PC, not in the cloud, in less than 10 minutes</a></li>
</ul>


<h3>LM Studio</h3>

<p><img src="https://lmstudio.ai/static/media/demo2.9df5a0e5a9f1d72715e0.gif" alt="" /></p>

<p><a href="https://lmstudio.ai/">LM Studio</a>是一个集成化的，用户友好的，界面漂亮的开源LLM应用程序，它集LLM下载运行和使用于一体，且有着非常好用的图形化的终端。缺点就是对硬件要求较高，且不能当成API来使用，无法与其他工具对接。</p>

<ul>
<li><a href="https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio">Run an LLM Locally with LM Studio</a></li>
<li><a href="https://digitaconnect.com/how-to-locally-run-a-llm-on-your-pc/">How to Locally Run a LLM on Your PC</a></li>
</ul>


<h3>GPT4All</h3>

<p><img src="https://img-blog.csdnimg.cn/img_convert/96470331467440dec2951abcac0dd195.png" alt="" /></p>

<p><a href="https://gpt4all.io/index.html">GPT4All</a>是一个与LM Studio类似的集成化的用户友好的工具。除了方便下载外，它也提供了好用的图形化终端来使用LLM。它除了可以使用下载的LLM以外，也支持API，并且除了正常的Chat以外，还能直接处理文档，也就是把文档当作LLM的输入。它对硬件的要求不像LM Studio那样高，缺点是对Mac似乎不太友好，因为它要求必须是最新版本的MacOS。</p>

<h2>总结</h2>

<p>本文介绍了几种使用起来非常方便的在本地运行LLM的方式，根据工具的特点，如果您使用的是Mac，或者想要与其他工具结合使用，那建议最好使用Ollama，毕竟它是对Mac最为友好的；如果硬件比较好就用LM Studio，否则的话就用GTP4ALL。</p>

<h2>参考资料</h2>

<ul>
<li><a href="https://semaphoreci.com/blog/local-llm">6 Ways For Running A Local LLM (how to use HuggingFace)</a></li>
<li><a href="https://www.infoworld.com/article/3705035/5-easy-ways-to-run-an-llm-locally.html">5 easy ways to run an LLM locally</a></li>
<li><a href="https://python.langchain.com/docs/guides/development/local_llms/">Run LLMs locally</a></li>
<li><a href="https://hackernoon.com/how-to-run-your-own-local-llm-updated-for-2024">How to Run Your Own Local LLM (Updated for 2024)</a></li>
<li><a href="https://kleiber.me/blog/2024/01/07/six-ways-running-llm-locally/">Six Ways of Running Large Language Models (LLMs) Locally (January 2024)</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
